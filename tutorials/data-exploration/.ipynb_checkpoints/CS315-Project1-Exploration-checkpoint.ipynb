{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134280eb-8ab6-43f8-bcbd-26b650a01ff0",
   "metadata": {},
   "source": [
    "# CS 315 Project 1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d79ba-ae0d-4475-befe-8f3384f8271e",
   "metadata": {},
   "source": [
    "Author: Eni Mustafaraj  \n",
    "Date: Feb 9, 2024\n",
    "\n",
    "This is a **data exploration** notebook. When we don't know the data and don't know yet what to do with them, we explore various aspects until we get a good sense of what the data is good for for.\n",
    "Then, we create another notebook that only contains the relevant analysis that we want to share with the world.\n",
    "\n",
    "**Table of Content**\n",
    "1. [Loading some data](#sec1)\n",
    "2. [Checking for missing values](#sec2)\n",
    "3. [Checking for unique values](#sec3)\n",
    "4. [Creating new columns](#sec4)\n",
    "5. [Aggregate all hashtags](#sec5)\n",
    "6. [Visualize Correlations with Seaborn](#sec6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8bd1d-fa5a-4fa1-9c16-2ea5f5600c7d",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## 1. Loading some data\n",
    "\n",
    "I will load into pandas the content of one of the files that contains one active test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c8277-4eee-48e3-9ae0-9fa886288c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries I might need in this notebook\n",
    "import os, csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b14d4c-8451-4f3e-b16d-7aea92648cc5",
   "metadata": {},
   "source": [
    "We know that our CSV files are in the `data` folder, so let's look them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180708eb-695f-4975-bc30-4d5fe3d30298",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data') # function lists the content of a directory\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06fd28e-6917-4350-89f1-42a20dfe8a51",
   "metadata": {},
   "source": [
    "For the moment, we can focus only on the files that contain all the videos, since the \"liked\" videos are just a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5036e-d00a-4b6f-b7df-9f75052fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyAllData = [f for f in files if 'data_all' in f] # filtering with list comprehension\n",
    "len(onlyAllData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f638ea3-d05a-4674-8732-83f1ce3dd8c5",
   "metadata": {},
   "source": [
    "Let's look at the content of one file, using pandas to read the CSV. Don't forget to give the relative path to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70287946-a7b6-4f7b-84c3-ca1fae26979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/02-09-05-18_like_by_hashtag_data_all_videos.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474adde-82b2-4d9f-a802-83c1a640ad6c",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. Checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbf9a9-b274-4743-968a-c0ac9e2391a1",
   "metadata": {},
   "source": [
    "We can notice above that the scraping didn't work well for the fields \"music\" and \"author\", because some values are shown as NaN (meaning, not a number or not available). We can use pandas method `isna` to count the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e0135-d813-4782-a1e0-88c9905324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[['music','author']].isna().sum() # first we find the missing values, then we count their total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a034e8ce-e78a-4e8c-9d11-b8134d518e22",
   "metadata": {},
   "source": [
    "Let's check another file and see if that is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbd09c-fa8f-43c3-aaae-6584934023b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/02-09-06-27_like_by_random_data_all_videos.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04642b9-17ad-4872-af55-f2371c6d0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[['music','author']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576eb6b-0757-48d1-9e10-9bec551d91bf",
   "metadata": {},
   "source": [
    "Looks good, this time none of the values for these two columns is missing. However, the \"hashtag\" column seem to have NaN values, but that is to be expected, because not all the posts have hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca45d0-8c61-4276-b8b3-c6ee8d5997d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['hashtag'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b7c6a-4088-489b-a8cb-6a7612d59714",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## 3. Checking for unique values\n",
    "\n",
    "Currently, the data scraping process is not collecting the video IDs which will let us know that each row is unique. Can we use another field for that purpose?\n",
    "\n",
    "First, let's find out the total number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba53940-28ea-4348-97f4-5d02990f3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069aa2b5-863d-472c-9e0e-dee2ec22298d",
   "metadata": {},
   "source": [
    "There are 108 rows (observations).  \n",
    "Let's find the unique \"author\" values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537a8fa-8170-4bfa-bbcf-183575463d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['author'].unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f7de7-7d2c-4c10-be84-1733be208f7e",
   "metadata": {},
   "source": [
    "It's not the same as the number of obsevations. We can check this with a boolean expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133dc05-9870-4754-9666-61540bc4106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['author'].unique().size == df1['author'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c0746-31f8-463f-991d-047ebdeca982",
   "metadata": {},
   "source": [
    "So, we confirmed that not all values are unique. The method `value_counts()` lets us see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0370f3-c49a-4627-be79-e4195e5dbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['author'].value_counts()[:5] # show the occurrences of the top 5 authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b673982-c1dc-4497-bd7e-5b9e93dc955b",
   "metadata": {},
   "source": [
    "We see that one author showed up many times. This is why we cannot rely on the field `author` as a unique identifier for a post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f152cfa-facb-49e0-aa6d-91d79ea26c86",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## 4. Creating new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5aca6-6837-4a33-9890-f9a6c6a58328",
   "metadata": {},
   "source": [
    "We will create a new column, postID, that is a combination of author + saves, in the hope that this will be a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d809cc5-b12c-4651-b2c6-318681c7671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPostID(row):\n",
    "    \"\"\"helper function: create a new value\"\"\"\n",
    "    return f\"{row['author']}_{row['saves']}\"\n",
    "\n",
    "df1['postID'] = df1.apply(createPostID, axis=1) # use axix=1 to process one row at a time\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e260a66-2dec-459b-8c34-b2c6b13124e1",
   "metadata": {},
   "source": [
    "**Some validation checks**\n",
    "\n",
    "Let's check that this new atribute is unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc3de7-45b6-4c02-89c1-bf056091cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['postID'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089740d3-f3df-4335-a38a-8ff7c767a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.postID.size == df1['postID'].unique().size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab2f12-ef10-4b35-a55b-fe64d60c37d7",
   "metadata": {},
   "source": [
    "We made sure that all our posts now have a unique ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b63e2-2a51-456b-88ce-48598757f84d",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## 5. Aggregate all hashtags\n",
    "\n",
    "In order to decide what hashtags to follow, I will read the hashtags of all my test runs and keep track of them to find the most popular onws. Initially, I will show how to get the list of hashtags from the dataframe column, **hashtag**.\n",
    "\n",
    "As we saw above, while not all posts have hashtags, most of them typically have multiple hashtags separated by comma. Thus, if we write a function to be applied to each cell of the dataframe, it needs to handle both these situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc955675-c349-48d9-a98c-b393faf82230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "allHashtagsCnt = Counter() # global variable that will be changed through the helper function\n",
    "\n",
    "def countAllHashtags(cell):\n",
    "    \"\"\"\n",
    "    Takes a string or a NaN value. Splits the srings to find hashtags, updates a Counter object\n",
    "    (a global variable) to keep track of all hashtags.\n",
    "    \"\"\"\n",
    "    if type(cell) == str: # avoid NaN values, which are float\n",
    "        htList = [el.strip() for el in cell.split(',')] # prepare individual hashtags\n",
    "        allHashtagsCnt.update(htList) # method update of Counter takes a list and updates all keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38922bc-1954-424f-bf4e-5004509eaf9d",
   "metadata": {},
   "source": [
    "Now I will apply this function to the column `hashtag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ac331-88e1-473b-a521-31b72e76cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df1['hashtag'].apply(countAllHashtags) # to avoid having to see the output, I use a dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c573c-3f78-4bab-b0d8-ea05ae57a2d5",
   "metadata": {},
   "source": [
    "Let's see the top hashtags using the `most_common` method of the Counter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbe757-86f1-48e6-b78e-3ddb0d16c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHashtagsCnt.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235823da-db41-4cbe-be52-e68330e2ddd2",
   "metadata": {},
   "source": [
    "This seems to work great, so let go through all the files I collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c34b91-55fa-47e0-8d67-58162772abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHashtagsCnt = Counter() # global variable; Counter dictionary\n",
    "\n",
    "for fN in onlyAllData: # folder with files that have all posts\n",
    "    path = os.path.join('data', fN) # create file path\n",
    "    df = pd.read_csv(path)\n",
    "    _ = df['hashtag'].apply(countAllHashtags)\n",
    "    print(f\"Processed {path}. Unique hashtags so far: {len(allHashtagsCnt)}, total occurrences: {sum(allHashtagsCnt.values())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cace5d4-24cb-40e4-900b-e5d65e2df83a",
   "metadata": {},
   "source": [
    "Let us see now the top 25 hashtgas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6b601-3e6f-4fb5-808b-3e0710163d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHashtagsCnt.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cc083-8bcd-46f4-9081-de6b6b0f0972",
   "metadata": {},
   "source": [
    "We see that some of the hashtags contain so-called \"percent encoded unicode characters\" (such as %E3%82%B7). Another name for them is URL encoding, becuase they are characters used in URLs, but that are not allowed, so they need to be encoded. Python's library `urllib` can parse these phrases for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cc2eb-3882-4fdb-a138-52de86344559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "urllib.parse.unquote('fyp%E3%82%B7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7031f-e2d8-45b9-a147-76c45442200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.parse.unquote('fyp%E3%82%B7%E3%82%9Aviral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970eeafa-9444-4968-9060-c732e20589f9",
   "metadata": {},
   "source": [
    "<a id=sec6></a>\n",
    "## 6. Visualize correlations with Seaborn\n",
    "\n",
    "I would like to see the correlation of the likes, shares, comments, saves for all the videos that we have collected so far. To do that, I will first create a big dataframe with observations from my 7 files with data. Then I will use seaborn's pairplot to see all correlations at once (pairplot generates scatterplots of every pair of variables).\n",
    "\n",
    "**Step 1: Concatenate the dataframes of all files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3f2e6-6055-4fb7-b0e1-54520082edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my files are in the list onlyAllData\n",
    "allDFs = []\n",
    "for fN in onlyAllData: # folder with files that have all posts\n",
    "    path = os.path.join('data', fN) # create file path\n",
    "    df = pd.read_csv(path) # create dataframe\n",
    "    allDFs.append(df)\n",
    "\n",
    "# concatenate all dataframes\n",
    "\n",
    "bigDF = pd.concat(allDFs, ignore_index=True)\n",
    "bigDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c0c6a-5a90-4e94-a97d-60e41f579288",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987e518-d22f-455e-875e-b28a3bee13d4",
   "metadata": {},
   "source": [
    "Let's look at the descriptive statistics of the interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e3b0e-5d55-46fa-a958-88731b55b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = bigDF[['likes','comments','shares','saves']]\n",
    "interactions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53ecb0-e060-4d87-ab76-9383c52bce44",
   "metadata": {},
   "source": [
    "Some of the values, like likes, are well in the millions, so the graphs will be difficult to \"see\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d917c2b-09c0-48e8-a8d1-74877fd1eb17",
   "metadata": {},
   "source": [
    "**Step 2: Pairplot for correlation with seaborn**\n",
    "\n",
    "Let's use `pairplot` of the shelf and then we can think about how to style it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51039a7-901d-4811-85c7-dcc9816c62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(interactions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d14197-be4e-41f9-bd08-eea6b1d956b4",
   "metadata": {},
   "source": [
    "Okay, there are a couple of issues with this graph: \n",
    "a) it's too big in size\n",
    "b) the plots are squashed due to outlier values\n",
    "c) the dots are too big\n",
    "\n",
    "Below, I'm doing a few things:\n",
    "1. taking the log values of every cell in our dataframe (to reduce the effect of outliers)\n",
    "2. adjusting the height of each subplot\n",
    "3. showing the regression line, in order to see if there is a linear relationship (we do this with `kind=\"reg\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2038a-f30c-4e9d-8611-644a0ce807dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logInter = np.log1p(interactions)\n",
    "sns.pairplot(logInter, kind='reg',\n",
    "             height=1.5, aspect=1, # control the height of each subplot\n",
    "             plot_kws={'scatter_kws': {'alpha': 0.5, 's': 5}, \n",
    "                       'line_kws': {'color': 'red'}}, \n",
    "                        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeba8c-6458-42a3-9270-e9d92789d93e",
   "metadata": {},
   "source": [
    "If my goal is only exploratory analysis, then this graph is good enough to show me the relationships between the variables. However, let's assume for the moment that I want to include such a graph into a publication. In that case, I might want to change many more little things to make it more readable. For example:\n",
    "\n",
    "1. I will change the range of x and y axes for each subplot, so that they are uniform\n",
    "2. I will change the color of the subplots (I don't like the defaultcolor)\n",
    "3. I will change the font size of all subplots because it looks too big (both labels and ticks)\n",
    "4. I will make the upper half of the matrix \"disappear\", becuase it's identical to the lower half.\n",
    "\n",
    "Making all these changes is not trivial, since pairplot is a wrapper function and one needs to go deep into its components to make changes. I gave instructions to a generative AI for the effects that I wanted (one by one) and I put together the code below. It was an iterative process, which is worth doing only for publication quality plots (such as research papers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a7d76-8a3e-43c7-929f-68691d445911",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(logInter, kind='reg',\n",
    "                 height=1.2, aspect=1, # control the height of each subplot\n",
    "                 plot_kws={'scatter_kws': {'alpha':0.2, 'color':'#6495ED','s': 3}, \n",
    "                           'line_kws': {'color': 'red', 'lw':1}}, \n",
    "                 diag_kws={'color':'#6495ED', 'edgecolor': '#6495ED'}\n",
    "                        )\n",
    "\n",
    "globalMin = logInter.min().min() # since there are two dimensions in the table, we need to take the min twice\n",
    "globalMax = logInter.max().max()\n",
    "\n",
    "# Set the same axis limits for all subplots\n",
    "for i in range(logInter.shape[1]): # iterate of the number of columns\n",
    "    for j in range(logInter.shape[1]):\n",
    "        g.axes[i, j].set_xlim(globalMin, globalMax)\n",
    "        g.axes[i, j].set_ylim(globalMin, globalMax)\n",
    "\n",
    "# Loop through the axes and set the font size\n",
    "for ax in g.axes.flatten():\n",
    "    # Set the font size for the x-axis labels\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=6)\n",
    "    # Set the font size for the y-axis labels\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=6)\n",
    "    # Set the font size for the tick labels on both axes\n",
    "    ax.tick_params(axis='both', labelsize=6)\n",
    "\n",
    "    # since we have log-scale, we should not show ticsk that are irrelevant\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Hide the upper triangle of plots\n",
    "    for i in range(len(g.axes)):\n",
    "        for j in range(i+1, len(g.axes)):\n",
    "            g.axes[i][j].set_visible(False)\n",
    "\n",
    "g.fig.suptitle('Relationship between variables (log-log scale)', \n",
    "               fontsize=10, fontname='monospace',\n",
    "               verticalalignment='top', y=1.05, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd784fb-3f9a-4109-9177-c6918655311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
